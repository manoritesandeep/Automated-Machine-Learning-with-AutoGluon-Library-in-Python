{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Task - Continous Value to Predict\n",
    "\n",
    "Eg: Future prices, Electricity loads, Test Scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement: Predict the continous label (Price) of a Uber ride based off list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'autogluon.core.dataset.TabularDataset'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   fare_amount        200000 non-null  float64\n",
      " 1   pickup_datetime    200000 non-null  object \n",
      " 2   pickup_longitude   200000 non-null  float64\n",
      " 3   pickup_latitude    200000 non-null  float64\n",
      " 4   dropoff_longitude  199999 non-null  float64\n",
      " 5   dropoff_latitude   199999 non-null  float64\n",
      " 6   passenger_count    200000 non-null  int64  \n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 10.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data = TabularDataset(\"data/uber/uber.csv\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fare_amount          0\n",
       "pickup_datetime      0\n",
       "pickup_longitude     0\n",
       "pickup_latitude      0\n",
       "dropoff_longitude    1\n",
       "dropoff_latitude     1\n",
       "passenger_count      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce data size ... \n",
    "small_df = data.sample(100000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split using sample\n",
    "train_data = small_df.sample(80000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = small_df.drop(train_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"uber_predictions\\\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.8.10\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22000\n",
      "Train Data Rows:    80000\n",
      "Train Data Columns: 6\n",
      "Label Column: fare_amount\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (499.0, -50.5, 11.33099, 10.00761)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    11836.1 MB\n",
      "\tTrain Data (Original)  Memory Usage: 9.6 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                        : 1 | ['passenger_count']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['pickup_datetime']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                  : 1 | ['passenger_count']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "\t9.1s = Fit runtime\n",
      "\t6 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.4 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 9.23s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.03125, Train Rows: 77500, Val Rows: 2500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-10.6214\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.36s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-11.1358\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 4.59603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.5863\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.13s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 4.50849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.493\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.91s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-4.5312\t = Validation score   (-root_mean_squared_error)\n",
      "\t82.2s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-4.459\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.83s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-5.0534\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.81s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-7.8433\t = Validation score   (-root_mean_squared_error)\n",
      "\t91.36s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-4.4931\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.82s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-5.8958\t = Validation score   (-root_mean_squared_error)\n",
      "\t193.66s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t-4.5682\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-4.3544\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 428.05s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"uber_predictions\\\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x12ac7b06b50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = \"uber_predictor\"\n",
    "\n",
    "predictor = TabularPredictor(label=\"fare_amount\", path=save_path)\n",
    "predictor.fit(train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** here is it reports validation scores in terms of negative root mean squared error.\n",
    "\n",
    "That's just a simple trick to stay with the convention that a higher value means a better performing model. And when we say higher, we mean closer to zero. I'm not talking about the absolute value of root mean squared error because recall root mean squared error.\n",
    "If it was equal to zero, that would mean the model fit it perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2  -4.354419       0.329937  127.923758                0.000000           0.225666            2       True         12\n",
      "1              CatBoost  -4.459048       0.006013   22.830516                0.006013          22.830516            1       True          6\n",
      "2              LightGBM  -4.493013       0.028220    2.912820                0.028220           2.912820            1       True          4\n",
      "3               XGBoost  -4.493071       0.016020    3.815557                0.016020           3.815557            1       True          9\n",
      "4       RandomForestMSE  -4.531197       0.120000   82.197350                0.120000          82.197350            1       True          5\n",
      "5         LightGBMLarge  -4.568189       0.011388    1.254153                0.011388           1.254153            1       True         11\n",
      "6            LightGBMXT  -4.586345       0.058995    4.128710                0.058995           4.128710            1       True          3\n",
      "7         ExtraTreesMSE  -5.053403       0.100688   11.813139                0.100688          11.813139            1       True          7\n",
      "8        NeuralNetTorch  -5.895770       0.017001  193.663711                0.017001         193.663711            1       True         10\n",
      "9       NeuralNetFastAI  -7.843284       0.055011   91.360762                0.055011          91.360762            1       True          8\n",
      "10       KNeighborsUnif -10.621416       0.013090    1.360521                0.013090           1.360521            1       True          1\n",
      "11       KNeighborsDist -11.135778       0.014700    0.237355                0.014700           0.237355            1       True          2\n",
      "Number of models trained: 12\n",
      "Types of models trained:\n",
      "{'CatBoostModel', 'TabularNeuralNetTorchModel', 'WeightedEnsembleModel', 'XTModel', 'NNFastAiTabularModel', 'KNNModel', 'RFModel', 'LGBModel', 'XGBoostModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])                : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "('int', [])                  : 1 | ['passenger_count']\n",
      "('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manor\\Downloads\\Automated Machine Learning with AutoGluon Library in Python\\autogluon_venv\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:138: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'KNeighborsUnif': 'KNNModel',\n",
       "  'KNeighborsDist': 'KNNModel',\n",
       "  'LightGBMXT': 'LGBModel',\n",
       "  'LightGBM': 'LGBModel',\n",
       "  'RandomForestMSE': 'RFModel',\n",
       "  'CatBoost': 'CatBoostModel',\n",
       "  'ExtraTreesMSE': 'XTModel',\n",
       "  'NeuralNetFastAI': 'NNFastAiTabularModel',\n",
       "  'XGBoost': 'XGBoostModel',\n",
       "  'NeuralNetTorch': 'TabularNeuralNetTorchModel',\n",
       "  'LightGBMLarge': 'LGBModel',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'KNeighborsUnif': -10.621415765896701,\n",
       "  'KNeighborsDist': -11.135777680585337,\n",
       "  'LightGBMXT': -4.586344810800017,\n",
       "  'LightGBM': -4.493013042225776,\n",
       "  'RandomForestMSE': -4.531197042019208,\n",
       "  'CatBoost': -4.459047922752713,\n",
       "  'ExtraTreesMSE': -5.053402970158025,\n",
       "  'NeuralNetFastAI': -7.843284148290898,\n",
       "  'XGBoost': -4.4930714918433745,\n",
       "  'NeuralNetTorch': -5.895769721896865,\n",
       "  'LightGBMLarge': -4.568189225889472,\n",
       "  'WeightedEnsemble_L2': -4.3544188139847195},\n",
       " 'model_best': 'WeightedEnsemble_L2',\n",
       " 'model_paths': {'KNeighborsUnif': 'uber_predictions\\\\models\\\\KNeighborsUnif\\\\',\n",
       "  'KNeighborsDist': 'uber_predictions\\\\models\\\\KNeighborsDist\\\\',\n",
       "  'LightGBMXT': 'uber_predictions\\\\models\\\\LightGBMXT\\\\',\n",
       "  'LightGBM': 'uber_predictions\\\\models\\\\LightGBM\\\\',\n",
       "  'RandomForestMSE': 'uber_predictions\\\\models\\\\RandomForestMSE\\\\',\n",
       "  'CatBoost': 'uber_predictions\\\\models\\\\CatBoost\\\\',\n",
       "  'ExtraTreesMSE': 'uber_predictions\\\\models\\\\ExtraTreesMSE\\\\',\n",
       "  'NeuralNetFastAI': 'uber_predictions\\\\models\\\\NeuralNetFastAI\\\\',\n",
       "  'XGBoost': 'uber_predictions\\\\models\\\\XGBoost\\\\',\n",
       "  'NeuralNetTorch': 'uber_predictions\\\\models\\\\NeuralNetTorch\\\\',\n",
       "  'LightGBMLarge': 'uber_predictions\\\\models\\\\LightGBMLarge\\\\',\n",
       "  'WeightedEnsemble_L2': 'uber_predictions\\\\models\\\\WeightedEnsemble_L2\\\\'},\n",
       " 'model_fit_times': {'KNeighborsUnif': 1.360520601272583,\n",
       "  'KNeighborsDist': 0.23735523223876953,\n",
       "  'LightGBMXT': 4.128709554672241,\n",
       "  'LightGBM': 2.912820339202881,\n",
       "  'RandomForestMSE': 82.197350025177,\n",
       "  'CatBoost': 22.83051633834839,\n",
       "  'ExtraTreesMSE': 11.813138961791992,\n",
       "  'NeuralNetFastAI': 91.36076211929321,\n",
       "  'XGBoost': 3.8155572414398193,\n",
       "  'NeuralNetTorch': 193.6637110710144,\n",
       "  'LightGBMLarge': 1.2541532516479492,\n",
       "  'WeightedEnsemble_L2': 0.22566580772399902},\n",
       " 'model_pred_times': {'KNeighborsUnif': 0.013090372085571289,\n",
       "  'KNeighborsDist': 0.014699935913085938,\n",
       "  'LightGBMXT': 0.05899548530578613,\n",
       "  'LightGBM': 0.028220176696777344,\n",
       "  'RandomForestMSE': 0.11999988555908203,\n",
       "  'CatBoost': 0.006013393402099609,\n",
       "  'ExtraTreesMSE': 0.10068821907043457,\n",
       "  'NeuralNetFastAI': 0.05501127243041992,\n",
       "  'XGBoost': 0.016019582748413086,\n",
       "  'NeuralNetTorch': 0.01700139045715332,\n",
       "  'LightGBMLarge': 0.011388063430786133,\n",
       "  'WeightedEnsemble_L2': 0.0},\n",
       " 'num_bag_folds': 0,\n",
       " 'max_stack_level': 2,\n",
       " 'model_hyperparams': {'KNeighborsUnif': {'weights': 'uniform'},\n",
       "  'KNeighborsDist': {'weights': 'distance'},\n",
       "  'LightGBMXT': {'learning_rate': 0.05, 'extra_trees': True},\n",
       "  'LightGBM': {'learning_rate': 0.05},\n",
       "  'RandomForestMSE': {'n_estimators': 300,\n",
       "   'max_leaf_nodes': 15000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'bootstrap': True,\n",
       "   'criterion': 'squared_error'},\n",
       "  'CatBoost': {'iterations': 10000,\n",
       "   'learning_rate': 0.05,\n",
       "   'random_seed': 0,\n",
       "   'allow_writing_files': False,\n",
       "   'eval_metric': 'RMSE'},\n",
       "  'ExtraTreesMSE': {'n_estimators': 300,\n",
       "   'max_leaf_nodes': 15000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'bootstrap': True,\n",
       "   'criterion': 'squared_error'},\n",
       "  'NeuralNetFastAI': {'layers': None,\n",
       "   'emb_drop': 0.1,\n",
       "   'ps': 0.1,\n",
       "   'bs': 'auto',\n",
       "   'lr': 0.01,\n",
       "   'epochs': 'auto',\n",
       "   'early.stopping.min_delta': 0.0001,\n",
       "   'early.stopping.patience': 20,\n",
       "   'smoothing': 0.0},\n",
       "  'XGBoost': {'n_estimators': 10000,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_jobs': -1,\n",
       "   'proc.max_category_levels': 100,\n",
       "   'objective': 'reg:squarederror',\n",
       "   'booster': 'gbtree'},\n",
       "  'NeuralNetTorch': {'num_epochs': 500,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'activation': 'relu',\n",
       "   'embedding_size_factor': 1.0,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'dropout_prob': 0.1,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.0003,\n",
       "   'weight_decay': 1e-06,\n",
       "   'proc.embed_min_categories': 4,\n",
       "   'proc.impute_strategy': 'median',\n",
       "   'proc.max_category_levels': 100,\n",
       "   'proc.skew_threshold': 0.99,\n",
       "   'use_ngram_features': False,\n",
       "   'num_layers': 4,\n",
       "   'hidden_size': 128,\n",
       "   'max_batch_size': 512,\n",
       "   'use_batchnorm': False,\n",
       "   'loss_function': 'auto'},\n",
       "  'LightGBMLarge': {'learning_rate': 0.03,\n",
       "   'num_leaves': 128,\n",
       "   'feature_fraction': 0.9,\n",
       "   'min_data_in_leaf': 5},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                   model  score_val  pred_time_val    fit_time  \\\n",
       " 0   WeightedEnsemble_L2  -4.354419       0.329937  127.923758   \n",
       " 1              CatBoost  -4.459048       0.006013   22.830516   \n",
       " 2              LightGBM  -4.493013       0.028220    2.912820   \n",
       " 3               XGBoost  -4.493071       0.016020    3.815557   \n",
       " 4       RandomForestMSE  -4.531197       0.120000   82.197350   \n",
       " 5         LightGBMLarge  -4.568189       0.011388    1.254153   \n",
       " 6            LightGBMXT  -4.586345       0.058995    4.128710   \n",
       " 7         ExtraTreesMSE  -5.053403       0.100688   11.813139   \n",
       " 8        NeuralNetTorch  -5.895770       0.017001  193.663711   \n",
       " 9       NeuralNetFastAI  -7.843284       0.055011   91.360762   \n",
       " 10       KNeighborsUnif -10.621416       0.013090    1.360521   \n",
       " 11       KNeighborsDist -11.135778       0.014700    0.237355   \n",
       " \n",
       "     pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                 0.000000           0.225666            2       True   \n",
       " 1                 0.006013          22.830516            1       True   \n",
       " 2                 0.028220           2.912820            1       True   \n",
       " 3                 0.016020           3.815557            1       True   \n",
       " 4                 0.120000          82.197350            1       True   \n",
       " 5                 0.011388           1.254153            1       True   \n",
       " 6                 0.058995           4.128710            1       True   \n",
       " 7                 0.100688          11.813139            1       True   \n",
       " 8                 0.017001         193.663711            1       True   \n",
       " 9                 0.055011          91.360762            1       True   \n",
       " 10                0.013090           1.360521            1       True   \n",
       " 11                0.014700           0.237355            1       True   \n",
       " \n",
       "     fit_order  \n",
       " 0          12  \n",
       " 1           6  \n",
       " 2           4  \n",
       " 3           9  \n",
       " 4           5  \n",
       " 5          11  \n",
       " 6           3  \n",
       " 7           7  \n",
       " 8          10  \n",
       " 9           8  \n",
       " 10          1  \n",
       " 11          2  }"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WeightedEnsemble_L2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"uber_predictor\"\n",
    "\n",
    "new_predictor = TabularPredictor.load(model_path)\n",
    "new_predictor.get_model_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_data['fare_amount']\n",
    "test_features = test_data.drop(columns=[\"fare_amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: root_mean_squared_error on test data: -4.455992447756374\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -4.455992447756374,\n",
      "    \"mean_squared_error\": -19.855868694461847,\n",
      "    \"mean_absolute_error\": -2.0334825034699437,\n",
      "    \"r2\": 0.7983413077253656,\n",
      "    \"pearsonr\": 0.8938195287881089,\n",
      "    \"median_absolute_error\": -1.237978076934814\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "y_preds = new_predictor.predict(test_features)\n",
    "metrics = new_predictor.evaluate_predictions(y_true=y_test, \n",
    "                                             y_pred=y_preds,\n",
    "                                             auxiliary_metrics=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression is a task when a model attempts to predict continous values (unlike categorical values, which is classification)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Metrics\n",
    "\n",
    "- MAE (Mean Absolute Error) - Mean of the absolute value of errors. MAE won't punish large errors. We want out error metrics to account for them. To fix it we use MSE\n",
    "- MSE (Mean Square Error) - Instead of absolute, we take square. Issue with MSE - Different units than y since it reports units of y-squared ($ --> $^2) which is wrong. To fix this issue we use RMSE\n",
    "- RMSE (Root Mean Square Error) - This is the root of the mean of the squared errors. Most popular (has same units as y). *What is a good value for RMSE?*. Context is everything, eg: A RMSE of $10 is fantastic for predicting the price of a house, but horrible for predicting the price of a candy bar. Again, the context of what your average value is for your label helps you understand the answer to the question of what is a good and acceptable value for your error metric.\n",
    "\n",
    "- As well as: R2, PearsonR, MedianAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 6 features using 5000 rows with 5 shuffle sets...\n",
      "\t101.01s\t= Expected runtime (20.2s per shuffle set)\n",
      "\t50.1s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <td>4.543023</td>\n",
       "      <td>0.462508</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>5</td>\n",
       "      <td>5.495334</td>\n",
       "      <td>3.590713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <td>4.113404</td>\n",
       "      <td>0.311613</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>5</td>\n",
       "      <td>4.755020</td>\n",
       "      <td>3.471787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_longitude</th>\n",
       "      <td>3.922065</td>\n",
       "      <td>0.486501</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>5</td>\n",
       "      <td>4.923776</td>\n",
       "      <td>2.920353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_latitude</th>\n",
       "      <td>3.051730</td>\n",
       "      <td>0.333700</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>5</td>\n",
       "      <td>3.738822</td>\n",
       "      <td>2.364638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_datetime</th>\n",
       "      <td>0.636072</td>\n",
       "      <td>0.127290</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>5</td>\n",
       "      <td>0.898165</td>\n",
       "      <td>0.373979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passenger_count</th>\n",
       "      <td>0.077510</td>\n",
       "      <td>0.161676</td>\n",
       "      <td>0.172045</td>\n",
       "      <td>5</td>\n",
       "      <td>0.410403</td>\n",
       "      <td>-0.255383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   importance    stddev   p_value  n  p99_high   p99_low\n",
       "dropoff_longitude    4.543023  0.462508  0.000013  5  5.495334  3.590713\n",
       "dropoff_latitude     4.113404  0.311613  0.000004  5  4.755020  3.471787\n",
       "pickup_longitude     3.922065  0.486501  0.000028  5  4.923776  2.920353\n",
       "pickup_latitude      3.051730  0.333700  0.000017  5  3.738822  2.364638\n",
       "pickup_datetime      0.636072  0.127290  0.000183  5  0.898165  0.373979\n",
       "passenger_count      0.077510  0.161676  0.172045  5  0.410403 -0.255383"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_predictor.feature_importance(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Settings with AutoGluon\n",
    "\n",
    "Inspect some advanced settings like smaller hyperparameter sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presets\n",
    "\n",
    "Autogluon comes with many presets that you can pass to the training routine.\n",
    "The training process will be adjusted depending on these presets.\n",
    "\n",
    "You can find the documentation about all presets here:\n",
    "https://auto.gluon.ai/stable/api/autogluon.tabular.TabularPredictor.fit.html#:~:text=num_bag_sets%20is%20specified.-,presets\n",
    "Currently the following presets are available:\n",
    "[‘best_quality’, ‘high_quality’, ‘good_quality’, ‘medium_quality’, ‘optimize_for_deployment’, ‘interpretable’, ‘ignore_text’]\n",
    "\n",
    "\n",
    "To get the best possible model you can use the *best_quality* preset - this will however drastically increase the training time.\n",
    "\n",
    "To reduce the training time we can exclude both Neural Networks via <br />\n",
    "**predictor.fit(train_data, presets=presets, excluded_model_types=[\"NN_TORCH\", \"FASTAI\"])**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  model  score_test  score_val  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2   -4.455992  -4.354419        3.680192       0.329937  127.923758                 0.005001                0.000000           0.225666            2       True         12\n",
      "1              CatBoost   -4.539429  -4.459048        0.084702       0.006013   22.830516                 0.084702                0.006013          22.830516            1       True          6\n",
      "2              LightGBM   -4.608628  -4.493013        0.284208       0.028220    2.912820                 0.284208                0.028220           2.912820            1       True          4\n",
      "3       RandomForestMSE   -4.619232  -4.531197        1.463759       0.120000   82.197350                 1.463759                0.120000          82.197350            1       True          5\n",
      "4         LightGBMLarge   -4.688755  -4.568189        0.119812       0.011388    1.254153                 0.119812                0.011388           1.254153            1       True         11\n",
      "5               XGBoost   -4.695420  -4.493071        0.126342       0.016020    3.815557                 0.126342                0.016020           3.815557            1       True          9\n",
      "6            LightGBMXT   -4.827842  -4.586345        0.630182       0.058995    4.128710                 0.630182                0.058995           4.128710            1       True          3\n",
      "7         ExtraTreesMSE   -5.289707  -5.053403        1.085998       0.100688   11.813139                 1.085998                0.100688          11.813139            1       True          7\n",
      "8        NeuralNetTorch   -6.014533  -5.895770        0.093430       0.017001  193.663711                 0.093430                0.017001         193.663711            1       True         10\n",
      "9        KNeighborsUnif  -10.735999 -10.621416        0.081508       0.013090    1.360521                 0.081508                0.013090           1.360521            1       True          1\n",
      "10       KNeighborsDist  -11.529670 -11.135778        0.063711       0.014700    0.237355                 0.063711                0.014700           0.237355            1       True          2\n",
      "11      NeuralNetFastAI -398.842741  -7.843284        0.364741       0.055011   91.360762                 0.364741                0.055011          91.360762            1       True          8\n"
     ]
    }
   ],
   "source": [
    "leaderboard_results = predictor.leaderboard(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-4.455992</td>\n",
       "      <td>-4.354419</td>\n",
       "      <td>3.680192</td>\n",
       "      <td>0.329937</td>\n",
       "      <td>127.923758</td>\n",
       "      <td>0.005001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225666</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>-4.539429</td>\n",
       "      <td>-4.459048</td>\n",
       "      <td>0.084702</td>\n",
       "      <td>0.006013</td>\n",
       "      <td>22.830516</td>\n",
       "      <td>0.084702</td>\n",
       "      <td>0.006013</td>\n",
       "      <td>22.830516</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>-4.608628</td>\n",
       "      <td>-4.493013</td>\n",
       "      <td>0.284208</td>\n",
       "      <td>0.028220</td>\n",
       "      <td>2.912820</td>\n",
       "      <td>0.284208</td>\n",
       "      <td>0.028220</td>\n",
       "      <td>2.912820</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestMSE</td>\n",
       "      <td>-4.619232</td>\n",
       "      <td>-4.531197</td>\n",
       "      <td>1.463759</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>82.197350</td>\n",
       "      <td>1.463759</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>82.197350</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LightGBMLarge</td>\n",
       "      <td>-4.688755</td>\n",
       "      <td>-4.568189</td>\n",
       "      <td>0.119812</td>\n",
       "      <td>0.011388</td>\n",
       "      <td>1.254153</td>\n",
       "      <td>0.119812</td>\n",
       "      <td>0.011388</td>\n",
       "      <td>1.254153</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>-4.695420</td>\n",
       "      <td>-4.493071</td>\n",
       "      <td>0.126342</td>\n",
       "      <td>0.016020</td>\n",
       "      <td>3.815557</td>\n",
       "      <td>0.126342</td>\n",
       "      <td>0.016020</td>\n",
       "      <td>3.815557</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LightGBMXT</td>\n",
       "      <td>-4.827842</td>\n",
       "      <td>-4.586345</td>\n",
       "      <td>0.630182</td>\n",
       "      <td>0.058995</td>\n",
       "      <td>4.128710</td>\n",
       "      <td>0.630182</td>\n",
       "      <td>0.058995</td>\n",
       "      <td>4.128710</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ExtraTreesMSE</td>\n",
       "      <td>-5.289707</td>\n",
       "      <td>-5.053403</td>\n",
       "      <td>1.085998</td>\n",
       "      <td>0.100688</td>\n",
       "      <td>11.813139</td>\n",
       "      <td>1.085998</td>\n",
       "      <td>0.100688</td>\n",
       "      <td>11.813139</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NeuralNetTorch</td>\n",
       "      <td>-6.014533</td>\n",
       "      <td>-5.895770</td>\n",
       "      <td>0.093430</td>\n",
       "      <td>0.017001</td>\n",
       "      <td>193.663711</td>\n",
       "      <td>0.093430</td>\n",
       "      <td>0.017001</td>\n",
       "      <td>193.663711</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNeighborsUnif</td>\n",
       "      <td>-10.735999</td>\n",
       "      <td>-10.621416</td>\n",
       "      <td>0.081508</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>1.360521</td>\n",
       "      <td>0.081508</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>1.360521</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>KNeighborsDist</td>\n",
       "      <td>-11.529670</td>\n",
       "      <td>-11.135778</td>\n",
       "      <td>0.063711</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.237355</td>\n",
       "      <td>0.063711</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.237355</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NeuralNetFastAI</td>\n",
       "      <td>-398.842741</td>\n",
       "      <td>-7.843284</td>\n",
       "      <td>0.364741</td>\n",
       "      <td>0.055011</td>\n",
       "      <td>91.360762</td>\n",
       "      <td>0.364741</td>\n",
       "      <td>0.055011</td>\n",
       "      <td>91.360762</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  score_test  score_val  pred_time_test  pred_time_val  \\\n",
       "0   WeightedEnsemble_L2   -4.455992  -4.354419        3.680192       0.329937   \n",
       "1              CatBoost   -4.539429  -4.459048        0.084702       0.006013   \n",
       "2              LightGBM   -4.608628  -4.493013        0.284208       0.028220   \n",
       "3       RandomForestMSE   -4.619232  -4.531197        1.463759       0.120000   \n",
       "4         LightGBMLarge   -4.688755  -4.568189        0.119812       0.011388   \n",
       "5               XGBoost   -4.695420  -4.493071        0.126342       0.016020   \n",
       "6            LightGBMXT   -4.827842  -4.586345        0.630182       0.058995   \n",
       "7         ExtraTreesMSE   -5.289707  -5.053403        1.085998       0.100688   \n",
       "8        NeuralNetTorch   -6.014533  -5.895770        0.093430       0.017001   \n",
       "9        KNeighborsUnif  -10.735999 -10.621416        0.081508       0.013090   \n",
       "10       KNeighborsDist  -11.529670 -11.135778        0.063711       0.014700   \n",
       "11      NeuralNetFastAI -398.842741  -7.843284        0.364741       0.055011   \n",
       "\n",
       "      fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
       "0   127.923758                 0.005001                0.000000   \n",
       "1    22.830516                 0.084702                0.006013   \n",
       "2     2.912820                 0.284208                0.028220   \n",
       "3    82.197350                 1.463759                0.120000   \n",
       "4     1.254153                 0.119812                0.011388   \n",
       "5     3.815557                 0.126342                0.016020   \n",
       "6     4.128710                 0.630182                0.058995   \n",
       "7    11.813139                 1.085998                0.100688   \n",
       "8   193.663711                 0.093430                0.017001   \n",
       "9     1.360521                 0.081508                0.013090   \n",
       "10    0.237355                 0.063711                0.014700   \n",
       "11   91.360762                 0.364741                0.055011   \n",
       "\n",
       "    fit_time_marginal  stack_level  can_infer  fit_order  \n",
       "0            0.225666            2       True         12  \n",
       "1           22.830516            1       True          6  \n",
       "2            2.912820            1       True          4  \n",
       "3           82.197350            1       True          5  \n",
       "4            1.254153            1       True         11  \n",
       "5            3.815557            1       True          9  \n",
       "6            4.128710            1       True          3  \n",
       "7           11.813139            1       True          7  \n",
       "8          193.663711            1       True         10  \n",
       "9            1.360521            1       True          1  \n",
       "10           0.237355            1       True          2  \n",
       "11          91.360762            1       True          8  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultra_fast_predictor=TabularPredictor(label=\"fare_amount\",\n",
    "                                      path=\"ultra_uber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer_limit specified, but infer_limit_batch_size was not specified. Setting infer_limit_batch_size=10000\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"ultra_uber\\\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.8.10\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22000\n",
      "Train Data Rows:    80000\n",
      "Train Data Columns: 6\n",
      "Label Column: fare_amount\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (499.0, -50.5, 11.33099, 10.00761)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    11606.36 MB\n",
      "\tTrain Data (Original)  Memory Usage: 9.6 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                        : 1 | ['passenger_count']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['pickup_datetime']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                  : 1 | ['passenger_count']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "\t8.9s = Fit runtime\n",
      "\t6 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.4 MB (0.1% of available memory)\n",
      "\t0.09ms\t= Feature Preprocessing Time (1 row | 10000 batch size)\n",
      "\t\tFeature Preprocessing requires 89.61% of the overall inference constraint (0.1ms)\n",
      "\t\t0.01ms inference time budget remaining for models...\n",
      "Data preprocessing and feature engineering runtime = 9.85s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.03125, Train Rows: 77500, Val Rows: 2500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-10.6214\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "\t2.61μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t2.61μs\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-11.1358\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "\t4.536μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t4.536μs\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 4.59603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.5863\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.23s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "\t0.036ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t0.036ms\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 4.50849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.493\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.37s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "\t0.02ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t0.02ms\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-4.5312\t = Validation score   (-root_mean_squared_error)\n",
      "\t107.96s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "\t0.033ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t0.033ms\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: CatBoost ...\n",
      "\t-4.459\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "\t0.71μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t0.71μs\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-5.0534\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.7s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "\t0.028ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t0.028ms\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-7.8433\t = Validation score   (-root_mean_squared_error)\n",
      "\t112.64s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "\t0.013ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t0.013ms\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: XGBoost ...\n",
      "\t-4.4931\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "\t4.594μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t4.594μs\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-5.8958\t = Validation score   (-root_mean_squared_error)\n",
      "\t237.85s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "\t9.57μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t9.57μs\t = Validation runtime (1 row | 10000 batch size)\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t-4.5682\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.34s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "\t6.856μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t6.856μs\t = Validation runtime (1 row | 10000 batch size)\n",
      "Removing 9/11 base models to satisfy inference constraint (constraint=9.867μs) ...\n",
      "\t0.158ms\t-> 0.122ms\t(LightGBMXT)\n",
      "\t0.122ms\t-> 0.102ms\t(LightGBM)\n",
      "\t0.102ms\t-> 0.069ms\t(RandomForestMSE)\n",
      "\t0.069ms\t-> 0.041ms\t(ExtraTreesMSE)\n",
      "\t0.041ms\t-> 0.029ms\t(NeuralNetFastAI)\n",
      "\t0.029ms\t-> 0.024ms\t(KNeighborsDist)\n",
      "\t0.024ms\t-> 0.022ms\t(KNeighborsUnif)\n",
      "\t0.022ms\t-> 0.012ms\t(NeuralNetTorch)\n",
      "\t0.012ms\t-> 5.305μs\t(LightGBMLarge)\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-4.4191\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "\t0.15μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
      "\t5.455μs\t = Validation runtime (1 row | 10000 batch size)\n",
      "AutoGluon training complete, total runtime = 535.45s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ultra_uber\\\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x12a8102a0a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ultra_fast_predictor.fit(train_data, infer_limit=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2  -4.419140       0.017547   34.270694                0.000000           0.144300            2       True         12\n",
      "1              CatBoost  -4.459048       0.003920   31.012999                0.003920          31.012999            1       True          6\n",
      "2              LightGBM  -4.493013       0.052990    4.370772                0.052990           4.370772            1       True          4\n",
      "3               XGBoost  -4.493071       0.013628    3.113395                0.013628           3.113395            1       True          9\n",
      "4       RandomForestMSE  -4.531197       0.156882  107.960097                0.156882         107.960097            1       True          5\n",
      "5         LightGBMLarge  -4.568189       0.018000    3.342052                0.018000           3.342052            1       True         11\n",
      "6            LightGBMXT  -4.586345       0.085999    6.228806                0.085999           6.228806            1       True          3\n",
      "7         ExtraTreesMSE  -5.053403       0.125509   14.701867                0.125509          14.701867            1       True          7\n",
      "8        NeuralNetTorch  -5.895770       0.047309  237.847161                0.047309         237.847161            1       True         10\n",
      "9       NeuralNetFastAI  -7.843284       0.033000  112.644372                0.033000         112.644372            1       True          8\n",
      "10       KNeighborsUnif -10.621416       0.017003    0.241281                0.017003           0.241281            1       True          1\n",
      "11       KNeighborsDist -11.135778       0.017668    0.286774                0.017668           0.286774            1       True          2\n",
      "Number of models trained: 12\n",
      "Types of models trained:\n",
      "{'CatBoostModel', 'TabularNeuralNetTorchModel', 'WeightedEnsembleModel', 'XTModel', 'NNFastAiTabularModel', 'KNNModel', 'RFModel', 'LGBModel', 'XGBoostModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])                : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "('int', [])                  : 1 | ['passenger_count']\n",
      "('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manor\\Downloads\\Automated Machine Learning with AutoGluon Library in Python\\autogluon_venv\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:138: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'KNeighborsUnif': 'KNNModel',\n",
       "  'KNeighborsDist': 'KNNModel',\n",
       "  'LightGBMXT': 'LGBModel',\n",
       "  'LightGBM': 'LGBModel',\n",
       "  'RandomForestMSE': 'RFModel',\n",
       "  'CatBoost': 'CatBoostModel',\n",
       "  'ExtraTreesMSE': 'XTModel',\n",
       "  'NeuralNetFastAI': 'NNFastAiTabularModel',\n",
       "  'XGBoost': 'XGBoostModel',\n",
       "  'NeuralNetTorch': 'TabularNeuralNetTorchModel',\n",
       "  'LightGBMLarge': 'LGBModel',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'KNeighborsUnif': -10.621415765896701,\n",
       "  'KNeighborsDist': -11.135777680585337,\n",
       "  'LightGBMXT': -4.586344810800017,\n",
       "  'LightGBM': -4.493013042225776,\n",
       "  'RandomForestMSE': -4.531197042019208,\n",
       "  'CatBoost': -4.459047922752713,\n",
       "  'ExtraTreesMSE': -5.053402970158025,\n",
       "  'NeuralNetFastAI': -7.843284148290898,\n",
       "  'XGBoost': -4.4930714918433745,\n",
       "  'NeuralNetTorch': -5.895769721896865,\n",
       "  'LightGBMLarge': -4.568189225889472,\n",
       "  'WeightedEnsemble_L2': -4.419140360544884},\n",
       " 'model_best': 'WeightedEnsemble_L2',\n",
       " 'model_paths': {'KNeighborsUnif': 'ultra_uber\\\\models\\\\KNeighborsUnif\\\\',\n",
       "  'KNeighborsDist': 'ultra_uber\\\\models\\\\KNeighborsDist\\\\',\n",
       "  'LightGBMXT': 'ultra_uber\\\\models\\\\LightGBMXT\\\\',\n",
       "  'LightGBM': 'ultra_uber\\\\models\\\\LightGBM\\\\',\n",
       "  'RandomForestMSE': 'ultra_uber\\\\models\\\\RandomForestMSE\\\\',\n",
       "  'CatBoost': 'ultra_uber\\\\models\\\\CatBoost\\\\',\n",
       "  'ExtraTreesMSE': 'ultra_uber\\\\models\\\\ExtraTreesMSE\\\\',\n",
       "  'NeuralNetFastAI': 'ultra_uber\\\\models\\\\NeuralNetFastAI\\\\',\n",
       "  'XGBoost': 'ultra_uber\\\\models\\\\XGBoost\\\\',\n",
       "  'NeuralNetTorch': 'ultra_uber\\\\models\\\\NeuralNetTorch\\\\',\n",
       "  'LightGBMLarge': 'ultra_uber\\\\models\\\\LightGBMLarge\\\\',\n",
       "  'WeightedEnsemble_L2': 'ultra_uber\\\\models\\\\WeightedEnsemble_L2\\\\'},\n",
       " 'model_fit_times': {'KNeighborsUnif': 0.24128055572509766,\n",
       "  'KNeighborsDist': 0.2867743968963623,\n",
       "  'LightGBMXT': 6.228806018829346,\n",
       "  'LightGBM': 4.370772123336792,\n",
       "  'RandomForestMSE': 107.96009683609009,\n",
       "  'CatBoost': 31.012998580932617,\n",
       "  'ExtraTreesMSE': 14.70186734199524,\n",
       "  'NeuralNetFastAI': 112.64437198638916,\n",
       "  'XGBoost': 3.1133952140808105,\n",
       "  'NeuralNetTorch': 237.84716057777405,\n",
       "  'LightGBMLarge': 3.3420522212982178,\n",
       "  'WeightedEnsemble_L2': 0.14429974555969238},\n",
       " 'model_pred_times': {'KNeighborsUnif': 0.017003297805786133,\n",
       "  'KNeighborsDist': 0.017667531967163086,\n",
       "  'LightGBMXT': 0.0859992504119873,\n",
       "  'LightGBM': 0.052989959716796875,\n",
       "  'RandomForestMSE': 0.15688228607177734,\n",
       "  'CatBoost': 0.0039196014404296875,\n",
       "  'ExtraTreesMSE': 0.12550878524780273,\n",
       "  'NeuralNetFastAI': 0.03300023078918457,\n",
       "  'XGBoost': 0.013627767562866211,\n",
       "  'NeuralNetTorch': 0.047309160232543945,\n",
       "  'LightGBMLarge': 0.017999649047851562,\n",
       "  'WeightedEnsemble_L2': 0.0},\n",
       " 'num_bag_folds': 0,\n",
       " 'max_stack_level': 2,\n",
       " 'model_hyperparams': {'KNeighborsUnif': {'weights': 'uniform'},\n",
       "  'KNeighborsDist': {'weights': 'distance'},\n",
       "  'LightGBMXT': {'learning_rate': 0.05, 'extra_trees': True},\n",
       "  'LightGBM': {'learning_rate': 0.05},\n",
       "  'RandomForestMSE': {'n_estimators': 300,\n",
       "   'max_leaf_nodes': 15000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'bootstrap': True,\n",
       "   'criterion': 'squared_error'},\n",
       "  'CatBoost': {'iterations': 10000,\n",
       "   'learning_rate': 0.05,\n",
       "   'random_seed': 0,\n",
       "   'allow_writing_files': False,\n",
       "   'eval_metric': 'RMSE'},\n",
       "  'ExtraTreesMSE': {'n_estimators': 300,\n",
       "   'max_leaf_nodes': 15000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'bootstrap': True,\n",
       "   'criterion': 'squared_error'},\n",
       "  'NeuralNetFastAI': {'layers': None,\n",
       "   'emb_drop': 0.1,\n",
       "   'ps': 0.1,\n",
       "   'bs': 'auto',\n",
       "   'lr': 0.01,\n",
       "   'epochs': 'auto',\n",
       "   'early.stopping.min_delta': 0.0001,\n",
       "   'early.stopping.patience': 20,\n",
       "   'smoothing': 0.0},\n",
       "  'XGBoost': {'n_estimators': 10000,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_jobs': -1,\n",
       "   'proc.max_category_levels': 100,\n",
       "   'objective': 'reg:squarederror',\n",
       "   'booster': 'gbtree'},\n",
       "  'NeuralNetTorch': {'num_epochs': 500,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'activation': 'relu',\n",
       "   'embedding_size_factor': 1.0,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'dropout_prob': 0.1,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.0003,\n",
       "   'weight_decay': 1e-06,\n",
       "   'proc.embed_min_categories': 4,\n",
       "   'proc.impute_strategy': 'median',\n",
       "   'proc.max_category_levels': 100,\n",
       "   'proc.skew_threshold': 0.99,\n",
       "   'use_ngram_features': False,\n",
       "   'num_layers': 4,\n",
       "   'hidden_size': 128,\n",
       "   'max_batch_size': 512,\n",
       "   'use_batchnorm': False,\n",
       "   'loss_function': 'auto'},\n",
       "  'LightGBMLarge': {'learning_rate': 0.03,\n",
       "   'num_leaves': 128,\n",
       "   'feature_fraction': 0.9,\n",
       "   'min_data_in_leaf': 5},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                   model  score_val  pred_time_val    fit_time  \\\n",
       " 0   WeightedEnsemble_L2  -4.419140       0.017547   34.270694   \n",
       " 1              CatBoost  -4.459048       0.003920   31.012999   \n",
       " 2              LightGBM  -4.493013       0.052990    4.370772   \n",
       " 3               XGBoost  -4.493071       0.013628    3.113395   \n",
       " 4       RandomForestMSE  -4.531197       0.156882  107.960097   \n",
       " 5         LightGBMLarge  -4.568189       0.018000    3.342052   \n",
       " 6            LightGBMXT  -4.586345       0.085999    6.228806   \n",
       " 7         ExtraTreesMSE  -5.053403       0.125509   14.701867   \n",
       " 8        NeuralNetTorch  -5.895770       0.047309  237.847161   \n",
       " 9       NeuralNetFastAI  -7.843284       0.033000  112.644372   \n",
       " 10       KNeighborsUnif -10.621416       0.017003    0.241281   \n",
       " 11       KNeighborsDist -11.135778       0.017668    0.286774   \n",
       " \n",
       "     pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                 0.000000           0.144300            2       True   \n",
       " 1                 0.003920          31.012999            1       True   \n",
       " 2                 0.052990           4.370772            1       True   \n",
       " 3                 0.013628           3.113395            1       True   \n",
       " 4                 0.156882         107.960097            1       True   \n",
       " 5                 0.018000           3.342052            1       True   \n",
       " 6                 0.085999           6.228806            1       True   \n",
       " 7                 0.125509          14.701867            1       True   \n",
       " 8                 0.047309         237.847161            1       True   \n",
       " 9                 0.033000         112.644372            1       True   \n",
       " 10                0.017003           0.241281            1       True   \n",
       " 11                0.017668           0.286774            1       True   \n",
       " \n",
       "     fit_order  \n",
       " 0          12  \n",
       " 1           6  \n",
       " 2           4  \n",
       " 3           9  \n",
       " 4           5  \n",
       " 5          11  \n",
       " 6           3  \n",
       " 7           7  \n",
       " 8          10  \n",
       " 9           8  \n",
       " 10          1  \n",
       " 11          2  }"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ultra_fast_predictor.fit_summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual enter Hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically what we do is, let Autogluon run on its own if all its default parameters choose the best models and then\n",
    "maybe start playing around with those hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one dict per model\n",
    "# hyperparameters_model = \n",
    "hyperparameters_NN_TORCH = {\"num_epochs\":1, \"learning_rate\":1}\n",
    "hyperparameters_XGB = {}    # empty dict for defaults\n",
    "# Dict of model parameters dicts\n",
    "hyperparameters = {\"NN_TORCH\":hyperparameters_NN_TORCH,\n",
    "                   \"XGB\":hyperparameters_XGB}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"manual_param\"\n"
     ]
    }
   ],
   "source": [
    "manual_predictor = TabularPredictor(label=\"fare_amount\",\n",
    "                                    path=\"manual_param\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"manual_param\\\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.8.10\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22000\n",
      "Train Data Rows:    80000\n",
      "Train Data Columns: 6\n",
      "Label Column: fare_amount\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (499.0, -50.5, 11.33099, 10.00761)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    11784.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 9.6 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                        : 1 | ['passenger_count']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['pickup_datetime']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                  : 1 | ['passenger_count']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "\t8.8s = Fit runtime\n",
      "\t6 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.4 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 8.87s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.03125, Train Rows: 77500, Val Rows: 2500\n",
      "Fitting 2 L1 models ...\n",
      "Fitting model: XGBoost ...\n",
      "\t-4.4931\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-63.3044\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.77s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-4.4931\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 15.09s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"manual_param\\\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x12a856a6850>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_predictor.fit(train_data, \n",
    "                     hyperparameters=hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manor\\Downloads\\Automated Machine Learning with AutoGluon Library in Python\\autogluon_venv\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:138: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0              XGBoost  -4.493071       0.011998  3.031473                0.011998           3.031473            1       True          1\n",
      "1  WeightedEnsemble_L2  -4.493071       0.011998  3.124309                0.000000           0.092836            2       True          3\n",
      "2       NeuralNetTorch -63.304356       0.015003  2.766997                0.015003           2.766997            1       True          2\n",
      "Number of models trained: 3\n",
      "Types of models trained:\n",
      "{'TabularNeuralNetTorchModel', 'XGBoostModel', 'WeightedEnsembleModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])                : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "('int', [])                  : 1 | ['passenger_count']\n",
      "('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'XGBoost': 'XGBoostModel',\n",
       "  'NeuralNetTorch': 'TabularNeuralNetTorchModel',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'XGBoost': -4.4930714918433745,\n",
       "  'NeuralNetTorch': -63.304355918035014,\n",
       "  'WeightedEnsemble_L2': -4.4930714918433745},\n",
       " 'model_best': 'WeightedEnsemble_L2',\n",
       " 'model_paths': {'XGBoost': 'manual_param\\\\models\\\\XGBoost\\\\',\n",
       "  'NeuralNetTorch': 'manual_param\\\\models\\\\NeuralNetTorch\\\\',\n",
       "  'WeightedEnsemble_L2': 'manual_param\\\\models\\\\WeightedEnsemble_L2\\\\'},\n",
       " 'model_fit_times': {'XGBoost': 3.031473398208618,\n",
       "  'NeuralNetTorch': 2.7669968605041504,\n",
       "  'WeightedEnsemble_L2': 0.09283566474914551},\n",
       " 'model_pred_times': {'XGBoost': 0.01199793815612793,\n",
       "  'NeuralNetTorch': 0.015002727508544922,\n",
       "  'WeightedEnsemble_L2': 0.0},\n",
       " 'num_bag_folds': 0,\n",
       " 'max_stack_level': 2,\n",
       " 'model_hyperparams': {'XGBoost': {'n_estimators': 10000,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_jobs': -1,\n",
       "   'proc.max_category_levels': 100,\n",
       "   'objective': 'reg:squarederror',\n",
       "   'booster': 'gbtree'},\n",
       "  'NeuralNetTorch': {'num_epochs': 1,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'activation': 'relu',\n",
       "   'embedding_size_factor': 1.0,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'dropout_prob': 0.1,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 1,\n",
       "   'weight_decay': 1e-06,\n",
       "   'proc.embed_min_categories': 4,\n",
       "   'proc.impute_strategy': 'median',\n",
       "   'proc.max_category_levels': 100,\n",
       "   'proc.skew_threshold': 0.99,\n",
       "   'use_ngram_features': False,\n",
       "   'num_layers': 4,\n",
       "   'hidden_size': 128,\n",
       "   'max_batch_size': 512,\n",
       "   'use_batchnorm': False,\n",
       "   'loss_function': 'auto'},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                  model  score_val  pred_time_val  fit_time  \\\n",
       " 0              XGBoost  -4.493071       0.011998  3.031473   \n",
       " 1  WeightedEnsemble_L2  -4.493071       0.011998  3.124309   \n",
       " 2       NeuralNetTorch -63.304356       0.015003  2.766997   \n",
       " \n",
       "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                0.011998           3.031473            1       True   \n",
       " 1                0.000000           0.092836            2       True   \n",
       " 2                0.015003           2.766997            1       True   \n",
       " \n",
       "    fit_order  \n",
       " 0          1  \n",
       " 1          3  \n",
       " 2          2  }"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_predictor.fit_summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoGluon: Presets and Deployment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Presets** are string codes that have built in combinations of hyperparameters or inference times for different models. For example, the preset best underscore quality will set the most accurate predictor as the best model, regardless of things like efficiency for inference time.\n",
    "Presets, however, can significantly impact predictive accuracy or memory footprint or inference latency of trained models and various other properties of the return predictor.\n",
    "\n",
    "*Available presets*: (should only choose one of the quality ones either go best high, good or medium)\n",
    "- \"best_quality\"\n",
    "- \"high_quality\"\n",
    "- \"good_quality\"\n",
    "- \"medium_quality\"\n",
    "- \"optimize_for_deployment\"\n",
    "- \"interpretable\"\n",
    "- \"ignore_text\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deployment**\n",
    "\n",
    "In the real world, you often only need the single best model to save on disk space. So before deployment, we often also want to retrain on all the data. If you wish to retrain on all the training data and not even use the validation, you can use **refit_full()** to retrain on all the data before deployment.\n",
    "\n",
    "And if you only want to save the minimum necessary for deployment, you can use the command, the predictor.clone_for_deployment(file_path) for the directory you're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: data/uber/uber.csv | Columns = 7 / 7 | Rows = 200000 -> 200000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TabularDataset(\"data/uber/uber.csv\")\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sample(20000)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.sample(19000)\n",
    "test_data = data.drop(train_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"new_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "presets = [\"medium_quality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TabularPredictor(label=\"fare_amount\",\n",
    "                             path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"new_model\\\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.8.10\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22000\n",
      "Train Data Rows:    19000\n",
      "Train Data Columns: 6\n",
      "Label Column: fare_amount\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (196.0, -3.0, 11.4791, 9.91419)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    11787.39 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.28 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                        : 1 | ['passenger_count']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['pickup_datetime']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                  : 1 | ['passenger_count']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "\t2.2s = Fit runtime\n",
      "\t6 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.52 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.25s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 17100, Val Rows: 1900\n",
      "Excluded Model Types: ['NN_TORCH', 'FASTAI']\n",
      "\tFound 'NN_TORCH' model in hyperparameters, but 'NN_TORCH' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'FASTAI' model in hyperparameters, but 'FASTAI' is present in `excluded_model_types` and will be removed.\n",
      "Fitting 9 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-10.8558\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-11.5397\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 4.6686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.6589\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.29s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t-4.1411\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-3.9075\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.65s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-3.9688\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-5.4676\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.42s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-4.1307\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t-4.0333\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-3.8089\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 43.65s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"new_model\\\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x12a856a6580>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit(train_data, presets=presets, \n",
    "              excluded_model_types=[\"NN_TORCH\", \"FASTAI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L2  -3.808940       0.092376  27.753736                0.000000           0.219824            2       True         10\n",
      "1      RandomForestMSE  -3.907481       0.077752  18.648419                0.077752          18.648419            1       True          5\n",
      "2             CatBoost  -3.968832       0.004016   7.257556                0.004016           7.257556            1       True          6\n",
      "3        LightGBMLarge  -4.033269       0.010607   1.627937                0.010607           1.627937            1       True          9\n",
      "4              XGBoost  -4.130710       0.008000   1.203883                0.008000           1.203883            1       True          8\n",
      "5             LightGBM  -4.141066       0.007002   1.495090                0.007002           1.495090            1       True          4\n",
      "6           LightGBMXT  -4.658902       0.146081   5.285334                0.146081           5.285334            1       True          3\n",
      "7        ExtraTreesMSE  -5.467560       0.071207   3.422928                0.071207           3.422928            1       True          7\n",
      "8       KNeighborsUnif -10.855805       0.020976   0.088573                0.020976           0.088573            1       True          1\n",
      "9       KNeighborsDist -11.539709       0.026967   0.050496                0.026967           0.050496            1       True          2\n",
      "Number of models trained: 10\n",
      "Types of models trained:\n",
      "{'CatBoostModel', 'WeightedEnsembleModel', 'XTModel', 'KNNModel', 'RFModel', 'LGBModel', 'XGBoostModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])                : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "('int', [])                  : 1 | ['passenger_count']\n",
      "('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manor\\Downloads\\Automated Machine Learning with AutoGluon Library in Python\\autogluon_venv\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:138: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'KNeighborsUnif': 'KNNModel',\n",
       "  'KNeighborsDist': 'KNNModel',\n",
       "  'LightGBMXT': 'LGBModel',\n",
       "  'LightGBM': 'LGBModel',\n",
       "  'RandomForestMSE': 'RFModel',\n",
       "  'CatBoost': 'CatBoostModel',\n",
       "  'ExtraTreesMSE': 'XTModel',\n",
       "  'XGBoost': 'XGBoostModel',\n",
       "  'LightGBMLarge': 'LGBModel',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'KNeighborsUnif': -10.855805443433303,\n",
       "  'KNeighborsDist': -11.539709115290115,\n",
       "  'LightGBMXT': -4.658901820617943,\n",
       "  'LightGBM': -4.141065831250388,\n",
       "  'RandomForestMSE': -3.9074813391312877,\n",
       "  'CatBoost': -3.968832178270391,\n",
       "  'ExtraTreesMSE': -5.467559628552042,\n",
       "  'XGBoost': -4.130709601885812,\n",
       "  'LightGBMLarge': -4.0332685712429,\n",
       "  'WeightedEnsemble_L2': -3.808940386437615},\n",
       " 'model_best': 'WeightedEnsemble_L2',\n",
       " 'model_paths': {'KNeighborsUnif': 'new_model\\\\models\\\\KNeighborsUnif\\\\',\n",
       "  'KNeighborsDist': 'new_model\\\\models\\\\KNeighborsDist\\\\',\n",
       "  'LightGBMXT': 'new_model\\\\models\\\\LightGBMXT\\\\',\n",
       "  'LightGBM': 'new_model\\\\models\\\\LightGBM\\\\',\n",
       "  'RandomForestMSE': 'new_model\\\\models\\\\RandomForestMSE\\\\',\n",
       "  'CatBoost': 'new_model\\\\models\\\\CatBoost\\\\',\n",
       "  'ExtraTreesMSE': 'new_model\\\\models\\\\ExtraTreesMSE\\\\',\n",
       "  'XGBoost': 'new_model\\\\models\\\\XGBoost\\\\',\n",
       "  'LightGBMLarge': 'new_model\\\\models\\\\LightGBMLarge\\\\',\n",
       "  'WeightedEnsemble_L2': 'new_model\\\\models\\\\WeightedEnsemble_L2\\\\'},\n",
       " 'model_fit_times': {'KNeighborsUnif': 0.08857297897338867,\n",
       "  'KNeighborsDist': 0.05049633979797363,\n",
       "  'LightGBMXT': 5.285333871841431,\n",
       "  'LightGBM': 1.4950895309448242,\n",
       "  'RandomForestMSE': 18.64841914176941,\n",
       "  'CatBoost': 7.257555723190308,\n",
       "  'ExtraTreesMSE': 3.4229276180267334,\n",
       "  'XGBoost': 1.203883171081543,\n",
       "  'LightGBMLarge': 1.6279370784759521,\n",
       "  'WeightedEnsemble_L2': 0.21982407569885254},\n",
       " 'model_pred_times': {'KNeighborsUnif': 0.02097606658935547,\n",
       "  'KNeighborsDist': 0.02696704864501953,\n",
       "  'LightGBMXT': 0.14608120918273926,\n",
       "  'LightGBM': 0.007001638412475586,\n",
       "  'RandomForestMSE': 0.07775235176086426,\n",
       "  'CatBoost': 0.004016399383544922,\n",
       "  'ExtraTreesMSE': 0.07120656967163086,\n",
       "  'XGBoost': 0.00800013542175293,\n",
       "  'LightGBMLarge': 0.010606765747070312,\n",
       "  'WeightedEnsemble_L2': 0.0},\n",
       " 'num_bag_folds': 0,\n",
       " 'max_stack_level': 2,\n",
       " 'model_hyperparams': {'KNeighborsUnif': {'weights': 'uniform'},\n",
       "  'KNeighborsDist': {'weights': 'distance'},\n",
       "  'LightGBMXT': {'learning_rate': 0.05, 'extra_trees': True},\n",
       "  'LightGBM': {'learning_rate': 0.05},\n",
       "  'RandomForestMSE': {'n_estimators': 300,\n",
       "   'max_leaf_nodes': 15000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'bootstrap': True,\n",
       "   'criterion': 'squared_error'},\n",
       "  'CatBoost': {'iterations': 10000,\n",
       "   'learning_rate': 0.05,\n",
       "   'random_seed': 0,\n",
       "   'allow_writing_files': False,\n",
       "   'eval_metric': 'RMSE'},\n",
       "  'ExtraTreesMSE': {'n_estimators': 300,\n",
       "   'max_leaf_nodes': 15000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'bootstrap': True,\n",
       "   'criterion': 'squared_error'},\n",
       "  'XGBoost': {'n_estimators': 10000,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_jobs': -1,\n",
       "   'proc.max_category_levels': 100,\n",
       "   'objective': 'reg:squarederror',\n",
       "   'booster': 'gbtree'},\n",
       "  'LightGBMLarge': {'learning_rate': 0.03,\n",
       "   'num_leaves': 128,\n",
       "   'feature_fraction': 0.9,\n",
       "   'min_data_in_leaf': 5},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                  model  score_val  pred_time_val   fit_time  \\\n",
       " 0  WeightedEnsemble_L2  -3.808940       0.092376  27.753736   \n",
       " 1      RandomForestMSE  -3.907481       0.077752  18.648419   \n",
       " 2             CatBoost  -3.968832       0.004016   7.257556   \n",
       " 3        LightGBMLarge  -4.033269       0.010607   1.627937   \n",
       " 4              XGBoost  -4.130710       0.008000   1.203883   \n",
       " 5             LightGBM  -4.141066       0.007002   1.495090   \n",
       " 6           LightGBMXT  -4.658902       0.146081   5.285334   \n",
       " 7        ExtraTreesMSE  -5.467560       0.071207   3.422928   \n",
       " 8       KNeighborsUnif -10.855805       0.020976   0.088573   \n",
       " 9       KNeighborsDist -11.539709       0.026967   0.050496   \n",
       " \n",
       "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                0.000000           0.219824            2       True   \n",
       " 1                0.077752          18.648419            1       True   \n",
       " 2                0.004016           7.257556            1       True   \n",
       " 3                0.010607           1.627937            1       True   \n",
       " 4                0.008000           1.203883            1       True   \n",
       " 5                0.007002           1.495090            1       True   \n",
       " 6                0.146081           5.285334            1       True   \n",
       " 7                0.071207           3.422928            1       True   \n",
       " 8                0.020976           0.088573            1       True   \n",
       " 9                0.026967           0.050496            1       True   \n",
       " \n",
       "    fit_order  \n",
       " 0         10  \n",
       " 1          5  \n",
       " 2          6  \n",
       " 3          9  \n",
       " 4          8  \n",
       " 5          4  \n",
       " 6          3  \n",
       " 7          7  \n",
       " 8          1  \n",
       " 9          2  }"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L2  -3.808940       0.092376  27.753736                0.000000           0.219824            2       True         10\n",
      "1      RandomForestMSE  -3.907481       0.077752  18.648419                0.077752          18.648419            1       True          5\n",
      "2             CatBoost  -3.968832       0.004016   7.257556                0.004016           7.257556            1       True          6\n",
      "3        LightGBMLarge  -4.033269       0.010607   1.627937                0.010607           1.627937            1       True          9\n",
      "4              XGBoost  -4.130710       0.008000   1.203883                0.008000           1.203883            1       True          8\n",
      "5             LightGBM  -4.141066       0.007002   1.495090                0.007002           1.495090            1       True          4\n",
      "6           LightGBMXT  -4.658902       0.146081   5.285334                0.146081           5.285334            1       True          3\n",
      "7        ExtraTreesMSE  -5.467560       0.071207   3.422928                0.071207           3.422928            1       True          7\n",
      "8       KNeighborsUnif -10.855805       0.020976   0.088573                0.020976           0.088573            1       True          1\n",
      "9       KNeighborsDist -11.539709       0.026967   0.050496                0.026967           0.050496            1       True          2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-3.808940</td>\n",
       "      <td>0.092376</td>\n",
       "      <td>27.753736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219824</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestMSE</td>\n",
       "      <td>-3.907481</td>\n",
       "      <td>0.077752</td>\n",
       "      <td>18.648419</td>\n",
       "      <td>0.077752</td>\n",
       "      <td>18.648419</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>-3.968832</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>7.257556</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>7.257556</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBMLarge</td>\n",
       "      <td>-4.033269</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>1.627937</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>1.627937</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>-4.130710</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.203883</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.203883</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>-4.141066</td>\n",
       "      <td>0.007002</td>\n",
       "      <td>1.495090</td>\n",
       "      <td>0.007002</td>\n",
       "      <td>1.495090</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LightGBMXT</td>\n",
       "      <td>-4.658902</td>\n",
       "      <td>0.146081</td>\n",
       "      <td>5.285334</td>\n",
       "      <td>0.146081</td>\n",
       "      <td>5.285334</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ExtraTreesMSE</td>\n",
       "      <td>-5.467560</td>\n",
       "      <td>0.071207</td>\n",
       "      <td>3.422928</td>\n",
       "      <td>0.071207</td>\n",
       "      <td>3.422928</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNeighborsUnif</td>\n",
       "      <td>-10.855805</td>\n",
       "      <td>0.020976</td>\n",
       "      <td>0.088573</td>\n",
       "      <td>0.020976</td>\n",
       "      <td>0.088573</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNeighborsDist</td>\n",
       "      <td>-11.539709</td>\n",
       "      <td>0.026967</td>\n",
       "      <td>0.050496</td>\n",
       "      <td>0.026967</td>\n",
       "      <td>0.050496</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_val  pred_time_val   fit_time  \\\n",
       "0  WeightedEnsemble_L2  -3.808940       0.092376  27.753736   \n",
       "1      RandomForestMSE  -3.907481       0.077752  18.648419   \n",
       "2             CatBoost  -3.968832       0.004016   7.257556   \n",
       "3        LightGBMLarge  -4.033269       0.010607   1.627937   \n",
       "4              XGBoost  -4.130710       0.008000   1.203883   \n",
       "5             LightGBM  -4.141066       0.007002   1.495090   \n",
       "6           LightGBMXT  -4.658902       0.146081   5.285334   \n",
       "7        ExtraTreesMSE  -5.467560       0.071207   3.422928   \n",
       "8       KNeighborsUnif -10.855805       0.020976   0.088573   \n",
       "9       KNeighborsDist -11.539709       0.026967   0.050496   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                0.000000           0.219824            2       True   \n",
       "1                0.077752          18.648419            1       True   \n",
       "2                0.004016           7.257556            1       True   \n",
       "3                0.010607           1.627937            1       True   \n",
       "4                0.008000           1.203883            1       True   \n",
       "5                0.007002           1.495090            1       True   \n",
       "6                0.146081           5.285334            1       True   \n",
       "7                0.071207           3.422928            1       True   \n",
       "8                0.020976           0.088573            1       True   \n",
       "9                0.026967           0.050496            1       True   \n",
       "\n",
       "   fit_order  \n",
       "0         10  \n",
       "1          5  \n",
       "2          6  \n",
       "3          9  \n",
       "4          8  \n",
       "5          4  \n",
       "6          3  \n",
       "7          7  \n",
       "8          1  \n",
       "9          2  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment\n",
    "\n",
    "new_predictor = TabularPredictor.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_predictor.fit_summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to deployment, I want to minimize and only take in and save to disk what I absolutely have to. So how do I actually set this up for deployment purposes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: KNeighborsUnif_FULL ...\n",
      "\t0.05s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: KNeighborsDist_FULL ...\n",
      "\t0.06s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMXT_FULL ...\n",
      "\t1.56s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_FULL ...\n",
      "\t0.4s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: RandomForestMSE_FULL ...\n",
      "\t21.09s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: CatBoost_FULL ...\n",
      "\t6.02s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: ExtraTreesMSE_FULL ...\n",
      "\t5.04s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: XGBoost_FULL ...\n",
      "\t0.56s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMLarge_FULL ...\n",
      "\t0.92s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t0.22s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 41.13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KNeighborsUnif': 'KNeighborsUnif_FULL',\n",
       " 'KNeighborsDist': 'KNeighborsDist_FULL',\n",
       " 'LightGBMXT': 'LightGBMXT_FULL',\n",
       " 'LightGBM': 'LightGBM_FULL',\n",
       " 'RandomForestMSE': 'RandomForestMSE_FULL',\n",
       " 'CatBoost': 'CatBoost_FULL',\n",
       " 'ExtraTreesMSE': 'ExtraTreesMSE_FULL',\n",
       " 'XGBoost': 'XGBoost_FULL',\n",
       " 'LightGBMLarge': 'LightGBMLarge_FULL',\n",
       " 'WeightedEnsemble_L2': 'WeightedEnsemble_L2_FULL'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.refit_full()  #.fit(data) --> refit_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloned TabularPredictor located in 'new_model\\' to 'final_model_for_deployment'.\n",
      "\tTo load the cloned predictor: predictor_clone = TabularPredictor.load(path=\"final_model_for_deployment\")\n",
      "Clone: Keeping minimum set of models required to predict with best model 'WeightedEnsemble_L2_FULL'...\n",
      "Deleting model KNeighborsUnif. All files under final_model_for_deployment\\models\\KNeighborsUnif\\ will be removed.\n",
      "Deleting model KNeighborsDist. All files under final_model_for_deployment\\models\\KNeighborsDist\\ will be removed.\n",
      "Deleting model LightGBMXT. All files under final_model_for_deployment\\models\\LightGBMXT\\ will be removed.\n",
      "Deleting model LightGBM. All files under final_model_for_deployment\\models\\LightGBM\\ will be removed.\n",
      "Deleting model RandomForestMSE. All files under final_model_for_deployment\\models\\RandomForestMSE\\ will be removed.\n",
      "Deleting model CatBoost. All files under final_model_for_deployment\\models\\CatBoost\\ will be removed.\n",
      "Deleting model ExtraTreesMSE. All files under final_model_for_deployment\\models\\ExtraTreesMSE\\ will be removed.\n",
      "Deleting model XGBoost. All files under final_model_for_deployment\\models\\XGBoost\\ will be removed.\n",
      "Deleting model LightGBMLarge. All files under final_model_for_deployment\\models\\LightGBMLarge\\ will be removed.\n",
      "Deleting model WeightedEnsemble_L2. All files under final_model_for_deployment\\models\\WeightedEnsemble_L2\\ will be removed.\n",
      "Deleting model KNeighborsUnif_FULL. All files under final_model_for_deployment\\models\\KNeighborsUnif_FULL\\ will be removed.\n",
      "Deleting model KNeighborsDist_FULL. All files under final_model_for_deployment\\models\\KNeighborsDist_FULL\\ will be removed.\n",
      "Deleting model LightGBMXT_FULL. All files under final_model_for_deployment\\models\\LightGBMXT_FULL\\ will be removed.\n",
      "Deleting model LightGBM_FULL. All files under final_model_for_deployment\\models\\LightGBM_FULL\\ will be removed.\n",
      "Deleting model ExtraTreesMSE_FULL. All files under final_model_for_deployment\\models\\ExtraTreesMSE_FULL\\ will be removed.\n",
      "Deleting model XGBoost_FULL. All files under final_model_for_deployment\\models\\XGBoost_FULL\\ will be removed.\n",
      "Clone: Removing artifacts unnecessary for prediction. NOTE: Clone can no longer fit new models, and most functionality except for predict and predict_proba will no longer work\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'final_model_for_deployment\\\\'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.clone_for_deployment(\"final_model_for_deployment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced AutoGluon: Custom Feature Engineering Pipeline\n",
    "\n",
    "https://auto.gluon.ai/stable/api/autogluon.features.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.features import AutoMLPipelineFeatureGenerator, DatetimeFeatureGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175730</th>\n",
       "      <td>3.5</td>\n",
       "      <td>2015-04-07 14:54:12 UTC</td>\n",
       "      <td>-73.968979</td>\n",
       "      <td>40.790646</td>\n",
       "      <td>-73.972145</td>\n",
       "      <td>40.784851</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159821</th>\n",
       "      <td>14.5</td>\n",
       "      <td>2011-05-20 15:17:24 UTC</td>\n",
       "      <td>-74.009799</td>\n",
       "      <td>40.703604</td>\n",
       "      <td>-73.997458</td>\n",
       "      <td>40.727364</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156548</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2012-03-26 18:17:00 UTC</td>\n",
       "      <td>-73.998673</td>\n",
       "      <td>40.734915</td>\n",
       "      <td>-73.977918</td>\n",
       "      <td>40.751484</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52401</th>\n",
       "      <td>11.3</td>\n",
       "      <td>2011-04-28 14:39:45 UTC</td>\n",
       "      <td>-73.991765</td>\n",
       "      <td>40.748796</td>\n",
       "      <td>-73.971204</td>\n",
       "      <td>40.751792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80055</th>\n",
       "      <td>6.5</td>\n",
       "      <td>2012-10-14 10:18:22 UTC</td>\n",
       "      <td>-73.996418</td>\n",
       "      <td>40.743102</td>\n",
       "      <td>-73.975587</td>\n",
       "      <td>40.733216</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fare_amount          pickup_datetime  pickup_longitude  \\\n",
       "175730          3.5  2015-04-07 14:54:12 UTC        -73.968979   \n",
       "159821         14.5  2011-05-20 15:17:24 UTC        -74.009799   \n",
       "156548          7.7  2012-03-26 18:17:00 UTC        -73.998673   \n",
       "52401          11.3  2011-04-28 14:39:45 UTC        -73.991765   \n",
       "80055           6.5  2012-10-14 10:18:22 UTC        -73.996418   \n",
       "\n",
       "        pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "175730        40.790646         -73.972145         40.784851                1  \n",
       "159821        40.703604         -73.997458         40.727364                1  \n",
       "156548        40.734915         -73.977918         40.751484                1  \n",
       "52401         40.748796         -73.971204         40.751792                1  \n",
       "80055         40.743102         -73.975587         40.733216                1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pipeline = AutoMLPipelineFeatureGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    12120.31 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.58 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 5 | ['fare_amount', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                        : 1 | ['passenger_count']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['pickup_datetime']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 5 | ['fare_amount', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                  : 1 | ['passenger_count']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "\t2.6s = Fit runtime\n",
      "\t7 features in original data used to generate 11 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.82 MB (0.0% of available memory)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_datetime.year</th>\n",
       "      <th>pickup_datetime.month</th>\n",
       "      <th>pickup_datetime.day</th>\n",
       "      <th>pickup_datetime.dayofweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175730</th>\n",
       "      <td>3.5</td>\n",
       "      <td>-73.968979</td>\n",
       "      <td>40.790646</td>\n",
       "      <td>-73.972145</td>\n",
       "      <td>40.784851</td>\n",
       "      <td>1</td>\n",
       "      <td>1428418452000000000</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159821</th>\n",
       "      <td>14.5</td>\n",
       "      <td>-74.009799</td>\n",
       "      <td>40.703604</td>\n",
       "      <td>-73.997458</td>\n",
       "      <td>40.727364</td>\n",
       "      <td>1</td>\n",
       "      <td>1305904644000000000</td>\n",
       "      <td>2011</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156548</th>\n",
       "      <td>7.7</td>\n",
       "      <td>-73.998673</td>\n",
       "      <td>40.734915</td>\n",
       "      <td>-73.977918</td>\n",
       "      <td>40.751484</td>\n",
       "      <td>1</td>\n",
       "      <td>1332785820000000000</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52401</th>\n",
       "      <td>11.3</td>\n",
       "      <td>-73.991765</td>\n",
       "      <td>40.748796</td>\n",
       "      <td>-73.971204</td>\n",
       "      <td>40.751792</td>\n",
       "      <td>1</td>\n",
       "      <td>1304001585000000000</td>\n",
       "      <td>2011</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80055</th>\n",
       "      <td>6.5</td>\n",
       "      <td>-73.996418</td>\n",
       "      <td>40.743102</td>\n",
       "      <td>-73.975587</td>\n",
       "      <td>40.733216</td>\n",
       "      <td>1</td>\n",
       "      <td>1350209902000000000</td>\n",
       "      <td>2012</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fare_amount  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "175730          3.5        -73.968979        40.790646         -73.972145   \n",
       "159821         14.5        -74.009799        40.703604         -73.997458   \n",
       "156548          7.7        -73.998673        40.734915         -73.977918   \n",
       "52401          11.3        -73.991765        40.748796         -73.971204   \n",
       "80055           6.5        -73.996418        40.743102         -73.975587   \n",
       "\n",
       "        dropoff_latitude  passenger_count      pickup_datetime  \\\n",
       "175730         40.784851                1  1428418452000000000   \n",
       "159821         40.727364                1  1305904644000000000   \n",
       "156548         40.751484                1  1332785820000000000   \n",
       "52401          40.751792                1  1304001585000000000   \n",
       "80055          40.733216                1  1350209902000000000   \n",
       "\n",
       "        pickup_datetime.year  pickup_datetime.month  pickup_datetime.day  \\\n",
       "175730                  2015                      4                    7   \n",
       "159821                  2011                      5                   20   \n",
       "156548                  2012                      3                   26   \n",
       "52401                   2011                      4                   28   \n",
       "80055                   2012                     10                   14   \n",
       "\n",
       "        pickup_datetime.dayofweek  \n",
       "175730                          1  \n",
       "159821                          4  \n",
       "156548                          0  \n",
       "52401                           3  \n",
       "80055                           6  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_features = feature_pipeline.fit_transform(train_data)\n",
    "transformed_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our own pipeline\n",
    "from autogluon.features import PipelineFeatureGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_custom_generator = PipelineFeatureGenerator(generators=[\n",
    "    DatetimeFeatureGenerator(features=[\"year\",\"month\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting PipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    12074.5 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.58 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tUnused Original Features (Count: 6): ['fare_amount', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 5 | ['fare_amount', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])   : 1 | ['passenger_count']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['pickup_datetime']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', ['datetime_as_int']) : 3 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month']\n",
      "\t1.9s = Fit runtime\n",
      "\t1 features in original data used to generate 3 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.61 MB (0.0% of available memory)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_datetime.year</th>\n",
       "      <th>pickup_datetime.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175730</th>\n",
       "      <td>1428418452000000000</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159821</th>\n",
       "      <td>1305904644000000000</td>\n",
       "      <td>2011</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156548</th>\n",
       "      <td>1332785820000000000</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52401</th>\n",
       "      <td>1304001585000000000</td>\n",
       "      <td>2011</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80055</th>\n",
       "      <td>1350209902000000000</td>\n",
       "      <td>2012</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160186</th>\n",
       "      <td>1390547340000000000</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147632</th>\n",
       "      <td>1255732680000000000</td>\n",
       "      <td>2009</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93719</th>\n",
       "      <td>1286391578000000000</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133357</th>\n",
       "      <td>1239485681000000000</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105755</th>\n",
       "      <td>1268791320000000000</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pickup_datetime  pickup_datetime.year  pickup_datetime.month\n",
       "175730  1428418452000000000                  2015                      4\n",
       "159821  1305904644000000000                  2011                      5\n",
       "156548  1332785820000000000                  2012                      3\n",
       "52401   1304001585000000000                  2011                      4\n",
       "80055   1350209902000000000                  2012                     10\n",
       "...                     ...                   ...                    ...\n",
       "160186  1390547340000000000                  2014                      1\n",
       "147632  1255732680000000000                  2009                     10\n",
       "93719   1286391578000000000                  2010                     10\n",
       "133357  1239485681000000000                  2009                      4\n",
       "105755  1268791320000000000                  2010                      3\n",
       "\n",
       "[19000 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_custom_generator.fit_transform(train_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogluon_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
